##### 历史

感知器、感知机

反向传播

RBM Initialization（受限玻尔兹曼机）（找学习的初始值）

2006 - GPU：使用GPU可以节省很多计算时间

##### 深度学习概述

**深度学习区别于机器学习**：在网络模型方面，使用的是神经网络（neural network）

**神经网络的连接方式**：全连接前馈神经网络（fully connect feedforward network）

- **深度学习的参数**：每一层的weight、bias（权重和偏差）

- 每一层每次计算之后，都会使用激活函数（sigmoid函数或其他函数）将其转化为0-1区间的一个数值。

  ![image-20211121162417840](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211121162417840.png)

 **神经网络的本质**：通过隐藏层进行特征转换。在最后一个隐藏层输出的就是一组新的特征（相当于黑箱操作），而对于输出层，其实是把前面的隐藏层的输出当做输入（经过特征提取得到的一组最好的特征），然后通过一个多分类器（可以是softmax函数）得到最后的输出y。

- **softmax函数**：类似于二分类问题，将多分类问题中的各个输出节点的输出范围映射到[0,1]，并且约束每个输出节点的输出值的和为1.

输入层、隐藏层、输出层

深度学习 = 隐藏层 * n

##### 模型评估

###### 1.损失函数

在深度学习中，不单单要计算某一个数据的损失，而是要计算整体所有训练数据的损失。将所有的训练数据的损失加起来，得到总体损失`L`。

###### 2.选择最优函数

方法：梯度下降法

具体流程：权重和偏差的集合是参数的集合。随机找一个初始值，计算一下每个参数对应的偏微分，得到的一个偏微分的集合∇L就是梯度。通过不断更新梯度得到新的参数，就能得到一组最好的参数使得损失函数`L`的值最小。

###### 3.反向传播 backpropagation (BP)

计算损失的方法，较优。

已有的框架：TensorFlow，theano，Pytorch...