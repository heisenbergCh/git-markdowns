#### 误差、梯度下降

##### 误差

误差的来源：bias和variance（偏差和方差）

<img src="https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119084659653.png" alt="image-20211119084659653" style="zoom: 67%;" />

以例说明：①bias：瞄准的位置偏离正确位置；②瞄准位置处的离散偏差。

 <img src="https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119085550514.png" alt="image-20211119085550514" style="zoom:67%;" />

简单的模型受到不同data的影响较小，而复杂的模型，data变化较大时，其模型方程式会变化很多。方差会很大，但平均值（偏差）与预期函数相差较小。

<img src="https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119090700272.png" alt="image-20211119090700272" style="zoom:67%;" />

每次选定方程，都相当于确定了一个model set，之后训练的每一个模型都只能在这个范围内寻找。当次数足够多时，即使偏差很大，方差也可以很小。

- 过拟合：误差来源于方差（训练集效果好，但测试集效果差）

- 欠拟合：误差来源于偏差（训练集效果差）

解决办法：

- 过拟合：①更多的数据（但收集数据困难）；②正则化regularization： 使曲线变得平滑。
- 欠拟合：优化模型。加入更多的函数，或者考虑更多次幂、更复杂的模型。

###### 模型选择

在训练集上表现较好的模型，不一定在测试集上表现地一样好。**因此不要过度地根据训练集调整模型，训练集包含的数据可能完整性不够，这可能会导致过拟合。**

###### 交叉验证

将public训练集分成多份。将所有的public训练数据集分为两部分，一部分作为训练集，一部分作为验证集。

- 如果训练效果较好，再用全部的public训练数据集训练模型，用public测试集进行测试。
- 此时的训练效果一般会较大（多大？），但此时不推荐回去调整参数、使模型在public测试集中表现更好。

![image-20211119144703078](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119144703078.png)

###### N-折交叉验证（N-fold cross validation）

![image-20211119145017975](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119145017975.png)

将训练集分成N份，再轮流进行训练测试，最后选取平均误差（Average Error）最小的模型，再用全部训练集训练该模型。

##### 梯度下降

![image-20211119145641382](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119145641382.png)

> 其中，n是学习率

1. 小心调整学习率learning rate

   对于二维和三维，可以图像化地理解学习率过低或过高带来的影响；但处于高维时，难以理解。所以转用参数的变化对Loss的变化

   ![image-20211119150245760](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119150245760.png)

   最开始时，步伐可以较大；当几次前进之后，离理想点较近，可以调小。

   ![image-20211119150506950](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119150506950.png)

   - 需要因材施教，对于不同的参数需要不同的学习率。

     

   - **Adagrad**：![image-20211119150709236](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119150709236.png)

     > root mean square：均方根；nt：与时间相关的学习率；gt：梯度
     >
     > ![image-20211119150952960](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119150952960.png)

     疑问：两项gt会给结果带来怎样的影响？

     ![image-20211119151650362](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119151650362.png)

     使用一次微分预测二次微分的值，降低运算量。

     ![image-20211119152423186](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119152423186.png)

2. stochastic gradient descent 随机梯度下降

   - 可以加快训练速度

   每次只选取一个例子，计算其Loss，计算梯度下降时也只考虑这一个，并更新数据。

   ![image-20211119152803155](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119152803155.png)

   对于单次数据及走向，可能不同，但走完所所有的例子后，表现与之前一样，但更快。

3. feature scaling 特征缩放/特征归一化、标准化

   ![image-20211119153336366](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211119153336366.png)

   使其分布范围变小。--**使某一个参数对结果的影响变小。防止一个过大，一个过小。**

   - 会使更新的过程变得有效率。

   > 特征缩放的方法：
   >
   > 1. 对于每一个数据的每一个维度，**减去平均值，并除以标准差**。结果中平均值为0，方差为1.

   

   ###### Grandient 的数学基础

   理论上，需要每次更新参数都使得损失函数减小，其前提条件是学习率足够小。

   梯度下降法会很容易陷入局部极值。

   

