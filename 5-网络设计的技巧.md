#### 网络设计的技巧

##### 1. optimization失败的时候应该怎么办

为什么会失败。

对于Loss的最后的值不满意。

某一处，梯度趋近于0。

但不仅仅可能是局部最小值，也可能是鞍状。

![image-20211123153343473](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211123153343473.png)

如何知道所处的位置（critical point）是哪一个？

- local minima暂时无法逃离
- saddle point可以鉴别，在其周围存在更低点

怎么知道Loss function的形状？【数学公式推导】

- 在ɵ'处的Loss fuction可以用**泰勒公式**表示：

  ![image-20211123154052186](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211123154052186.png)

  ![image-20211123154239253](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211123154239253.png)

  如何根据后面这一项判断critical point处的情况？：

  - positive definite 正定
  - negative definite 负定 
  - eigen value 特征值

  ![image-20211123155114544](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211123155114544.png)

  对于saddle point，我们虽然不用担心g无法继续提供**参数优化的方向**，但仍需要通过其他途径得到优化方向，比如`H`：

  ![image-20211123160329271](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211123160329271.png)

  （例子）

  ![image-20211123160501816](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211123160501816.png)

  - 但实际中不太可能用这种方法，因为运算量过大。后续会有其他的方法逃离saddle point

- saddle point 和 local minima 哪一个更常见？【更多参数，更高维度】

  ![image-20211123161349380](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211123161349380.png)

  所以local minima并没有那么常见

##### 2. batch（批次）和momentum（动量）的训练技巧

将数据集分为一个一个的batch，再计算Loss和gradient，再更新参数。一轮（epoch）看完所有的batch。

每一个epoch开始前会shuffle（洗牌），打乱后重新分batch。

###### 为什么要分batch?

- ![image-20211123162041375](https://gitee.com/yijia7590jfz/typora-img/raw/master/image-20211123162041375.png)

  > 左边的蓄力时间较长，但方向更稳定（powerful）
  >
  > 右边蓄力的时间更短，但方向更不稳定（noisy）
  >
  > 如果考虑平行运算，并不一定左边的时间更长









MNIST（Mixed National Institute of Standards and Technology），手写数字辨识。